{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/grkmkola/Desktop/Projects/mlops-proje/kidney-disease-classification/research'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/grkmkola/Desktop/Projects/mlops-proje/kidney-disease-classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grkmkola/miniconda3/envs/kidney/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/grkmkola/Desktop/Projects/mlops-proje/kidney-disease-classification'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    last_model_path: Path\n",
    "    best_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    tensorboard_log_dir: Path\n",
    "    training_data: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_augmentation: bool\n",
    "    params_image_size: list\n",
    "    params_early_stopping_patience: int\n",
    "    params_learning_rate: float\n",
    "    params_random_state: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils import read_yaml, create_directories, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH,\n",
    "        ) -> None:\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories(\n",
    "            [\n",
    "                self.config.artifacts_root,\n",
    "                self.config.training.root_dir,\n",
    "                self.config.training.tensorboard_log_dir,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_training_config(self):\n",
    "        config = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        data_ingestion = self.config.data_ingestion\n",
    "\n",
    "        params = self.params\n",
    "\n",
    "        training_data = os.path.join(\n",
    "            data_ingestion.unzip_dir,\n",
    "            \"CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone\"    \n",
    "        )\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            last_model_path=Path(config.last_model_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            tensorboard_log_dir=Path(config.tensorboard_log_dir),\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_augmentation=params.AUGMENTATION,\n",
    "            params_image_size=params.IMAGE_SIZE,\n",
    "            params_early_stopping_patience=params.EARLY_STOPPING_PATIENCE,\n",
    "            params_learning_rate=params.LEARNING_RATE,\n",
    "            params_random_state=params.RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_base_model(self):\n",
    "        self.model = torch.load(self.config.updated_base_model_path)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train_valid_test_loader(self):\n",
    "        basic_transform = transforms.Compose([\n",
    "            transforms.Resize(self.config.params_image_size[:-1]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        if self.config.params_augmentation:\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.RandomRotation(40),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomResizedCrop(self.config.params_image_size[0], scale=(0.8, 1.0)),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                basic_transform\n",
    "            ])\n",
    "        else:\n",
    "            train_transform = basic_transform\n",
    "\n",
    "        full_dataset = datasets.ImageFolder(self.config.training_data, transform=train_transform)\n",
    "\n",
    "        total_size = len(full_dataset)\n",
    "        train_size = int(0.7 * total_size)  # 70% for training\n",
    "        valid_size = int(0.15 * total_size)  # 15% for validation\n",
    "        test_size = total_size - train_size - valid_size  # 15% for testing\n",
    "\n",
    "        # Ensure reproducibility\n",
    "        generator = torch.Generator().manual_seed(self.config.params_random_state)\n",
    "\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(full_dataset, [train_size, valid_size, test_size], generator=generator)\n",
    "\n",
    "        # Apply transforms\n",
    "        train_dataset.dataset.transform = train_transform\n",
    "        valid_dataset.dataset.transform = basic_transform\n",
    "        test_dataset.dataset.transform = basic_transform\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.config.params_batch_size, shuffle=True, num_workers=4)\n",
    "        self.valid_loader = DataLoader(valid_dataset, batch_size=self.config.params_batch_size, shuffle=False, num_workers=4)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=self.config.params_batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "        logger.info(f\"Number of training samples: {len(train_dataset)}\")\n",
    "        logger.info(f\"Number of validation samples: {len(valid_dataset)}\")\n",
    "        logger.info(f\"Number of test samples: {len(test_dataset)}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: nn.Module):\n",
    "        torch.save(model, path)\n",
    "\n",
    "    def train(self):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.config.params_learning_rate)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.config.params_learning_rate, steps_per_epoch=len(self.train_loader), epochs=self.config.params_epochs)\n",
    "        best_valid_loss = float('inf')\n",
    "        early_stopping_counter = 0\n",
    "\n",
    "        for epoch in range(self.config.params_epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.config.params_epochs} [Train]')\n",
    "            for inputs, labels in train_pbar:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                train_loss += loss.item() * inputs.size(0)\n",
    "                train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            train_loss = train_loss / len(self.train_loader.dataset)\n",
    "\n",
    "            self.model.eval()\n",
    "            valid_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            all_labels = []\n",
    "            all_predictions = []\n",
    "            valid_pbar = tqdm(self.valid_loader, desc=f'Epoch {epoch+1}/{self.config.params_epochs} [Valid]')\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in valid_pbar:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    valid_loss += loss.item() * inputs.size(0)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    \n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    all_predictions.extend(predicted.cpu().numpy())\n",
    "                    \n",
    "                    valid_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            valid_loss = valid_loss / len(self.valid_loader.dataset)\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Calculate precision, recall, and F1 score\n",
    "            precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "            recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "            f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "            logger.info(f'Epoch {epoch+1}/{self.config.params_epochs}, '\n",
    "                         f'Train Loss: {train_loss:.4f}, '\n",
    "                         f'Valid Loss: {valid_loss:.4f}, '\n",
    "                         f'Valid Accuracy: {accuracy:.2f}%, '\n",
    "                         f'Precision: {precision:.4f}, '\n",
    "                         f'Recall: {recall:.4f}, '\n",
    "                         f'F1 Score: {f1:.4f}')\n",
    "\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                early_stopping_counter = 0\n",
    "                self.save_model(path=self.config.last_model_path, model=self.model)\n",
    "                logger.info(f'Saved best model with valid loss: {valid_loss:.4f}')\n",
    "                \n",
    "                # Save best model separately\n",
    "                best_model_path = self.config.last_model_path.parent / \"best_model.pth\"\n",
    "                self.save_model(path=best_model_path, model=self.model)\n",
    "                logger.info(f'Saved best model separately at: {best_model_path}')\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= self.config.params_early_stopping_patience:\n",
    "                    logger.info('Early stopping triggered')\n",
    "                    break\n",
    "\n",
    "        logger.info(f'Training completed. Best validation loss: {best_valid_loss:.4f}')\n",
    "        logger.info(f'Best model saved at: {self.config.last_model_path.parent / \"best_model.pth\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-23 16:57:11,098: INFO: utils: yaml file config/config.yaml loaded successfully:]\n",
      "[2024-07-23 16:57:11,105: INFO: utils: yaml file params.yaml loaded successfully:]\n",
      "[2024-07-23 16:57:11,107: INFO: utils: created directory at: artifacts:]\n",
      "[2024-07-23 16:57:11,110: INFO: utils: created directory at: artifacts/training:]\n",
      "[2024-07-23 16:57:11,113: INFO: utils: created directory at: artifacts/training/tensorboard_logs:]\n",
      "[2024-07-23 16:57:12,672: INFO: 4114949571: Number of training samples: 8712:]\n",
      "[2024-07-23 16:57:12,697: INFO: 4114949571: Number of validation samples: 1866:]\n",
      "[2024-07-23 16:57:12,699: INFO: 4114949571: Number of test samples: 1868:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 [Train]: 100%|██████████| 545/545 [01:00<00:00,  8.95it/s, loss=0.4473]\n",
      "Epoch 1/1 [Valid]: 100%|██████████| 117/117 [00:12<00:00,  9.23it/s, loss=0.1166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-23 16:58:26,356: INFO: 4114949571: Epoch 1/1, Train Loss: 0.7426, Valid Loss: 0.2509, Valid Accuracy: 91.69%, Precision: 0.9167, Recall: 0.9169, F1 Score: 0.9151:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-23 16:58:26,684: INFO: 4114949571: Saved best model with valid loss: 0.2509:]\n",
      "[2024-07-23 16:58:27,060: INFO: 4114949571: Saved best model separately at: artifacts/training/best_model.pth:]\n",
      "[2024-07-23 16:58:27,064: INFO: 4114949571: Training completed. Best validation loss: 0.2509:]\n",
      "[2024-07-23 16:58:27,065: INFO: 4114949571: Best model saved at: artifacts/training/best_model.pth:]\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager().get_training_config()\n",
    "training = Training(config)\n",
    "training.get_base_model()\n",
    "training.train_valid_test_loader()\n",
    "training.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kidney",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
