{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/grkmkola/Desktop/Projects/mlops-proje/kidney-disease-classification/research'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/grkmkola/Desktop/Projects/mlops-proje/kidney-disease-classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grkmkola/miniconda3/envs/kidney/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/grkmkola/Desktop/Projects/mlops-proje/kidney-disease-classification'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    tensorboard_log_dir: Path\n",
    "    training_data: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_augmentation: bool\n",
    "    params_image_size: list\n",
    "    params_early_stopping_patience: int\n",
    "    params_learning_rate: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH,\n",
    "        ) -> None:\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories(\n",
    "            [\n",
    "                self.config.artifacts_root,\n",
    "                self.config.training.root_dir,\n",
    "                self.config.training.tensorboard_log_dir,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_training_config(self):\n",
    "        config = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        data_ingestion = self.config.data_ingestion\n",
    "\n",
    "        params = self.params\n",
    "\n",
    "        training_data = os.path.join(\n",
    "            data_ingestion.unzip_dir,\n",
    "            \"CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone\"    \n",
    "        )\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            trained_model_path=Path(config.trained_model_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_augmentation=params.AUGMENTATION,\n",
    "            params_image_size=params.IMAGE_SIZE,\n",
    "            params_early_stopping_patience=params.EARLY_STOPPING_PATIENCE,\n",
    "            params_learning_rate=params.LEARNING_RATE\n",
    "        )\n",
    "        \n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.setup_logging()\n",
    "        self.writer = SummaryWriter(log_dir=self.config.tensorboard_log_dir)\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self.logger = logging.getLogger()\n",
    "\n",
    "    def get_base_model(self):\n",
    "        self.model = torch.load(self.config.updated_base_model_path)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train_valid_loader(self):\n",
    "        basic_transform = transforms.Compose([\n",
    "            transforms.Resize(self.config.params_image_size[:-1]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        if self.config.params_augmentation:\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.RandomRotation(40),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomResizedCrop(self.config.params_image_size[0], scale=(0.8, 1.0)),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                basic_transform\n",
    "            ])\n",
    "        else:\n",
    "            train_transform = basic_transform\n",
    "\n",
    "        full_dataset = datasets.ImageFolder(self.config.training_data, transform=train_transform)\n",
    "\n",
    "        total_size = len(full_dataset)\n",
    "        train_size = int(0.8 * total_size)\n",
    "        valid_size = total_size - train_size\n",
    "\n",
    "        train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        train_dataset.dataset.transform = train_transform\n",
    "        valid_dataset.dataset.transform = basic_transform\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.config.params_batch_size, shuffle=True, num_workers=4)\n",
    "        self.valid_loader = DataLoader(valid_dataset, batch_size=self.config.params_batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "        self.logger.info(f\"Number of training samples: {len(train_dataset)}\")\n",
    "        self.logger.info(f\"Number of validation samples: {len(valid_dataset)}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: nn.Module):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "    def find_best_lr(self):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=1e-7)\n",
    "\n",
    "        lrs = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        smoothing = 0.05\n",
    "\n",
    "        for inputs, labels in tqdm(self.train_loader, desc=\"Finding best LR\"):\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(optimizer)\n",
    "            self.scaler.update()\n",
    "\n",
    "            # Track the learning rate and loss\n",
    "            lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "            losses.append(loss.item())\n",
    "            optimizer.param_groups[0][\"lr\"] *= 1.1  # Increase the learning rate\n",
    "\n",
    "            # Smooth the loss for stability\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "            else:\n",
    "                loss = smoothing * loss.item() + (1 - smoothing) * best_loss\n",
    "\n",
    "        self.writer.add_scalar('LR Finder/lr', lrs, 0)\n",
    "        self.writer.add_scalar('LR Finder/loss', losses, 0)\n",
    "        best_lr = lrs[losses.index(min(losses))]\n",
    "        return best_lr\n",
    "\n",
    "    def train(self):\n",
    "        best_lr = self.find_best_lr()\n",
    "        self.logger.info(f\"Using best LR: {best_lr}\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=best_lr)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=best_lr, steps_per_epoch=len(self.train_loader), epochs=self.config.params_epochs)\n",
    "        best_valid_loss = float('inf')\n",
    "        early_stopping_counter = 0\n",
    "\n",
    "        for epoch in range(self.config.params_epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.config.params_epochs} [Train]')\n",
    "            for inputs, labels in train_pbar:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with autocast():\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(optimizer)\n",
    "                self.scaler.update()\n",
    "                scheduler.step()\n",
    "\n",
    "                train_loss += loss.item() * inputs.size(0)\n",
    "                train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            train_loss = train_loss / len(self.train_loader.dataset)\n",
    "            self.writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "            self.model.eval()\n",
    "            valid_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            all_labels = []\n",
    "            all_preds = []\n",
    "            valid_pbar = tqdm(self.valid_loader, desc=f'Epoch {epoch+1}/{self.config.params_epochs} [Valid]')\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in valid_pbar:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    valid_loss += loss.item() * inputs.size(0)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    valid_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "            valid_loss = valid_loss / len(self.valid_loader.dataset)\n",
    "            accuracy = 100 * correct / total\n",
    "            precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "            recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "            f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "            self.writer.add_scalar('Loss/valid', valid_loss, epoch)\n",
    "            self.writer.add_scalar('Accuracy/valid', accuracy, epoch)\n",
    "            self.writer.add_scalar('Precision/valid', precision, epoch)\n",
    "            self.writer.add_scalar('Recall/valid', recall, epoch)\n",
    "            self.writer.add_scalar('F1-Score/valid', f1, epoch)\n",
    "\n",
    "            self.logger.info(f'Epoch {epoch+1}/{self.config.params_epochs}, '\n",
    "                             f'Train Loss: {train_loss:.4f}, '\n",
    "                             f'Valid Loss: {valid_loss:.4f}, '\n",
    "                             f'Valid Accuracy: {accuracy:.2f}%, '\n",
    "                             f'Precision: {precision:.4f}, '\n",
    "                             f'Recall: {recall:.4f}, '\n",
    "                             f'F1-Score: {f1:.4f}')\n",
    "\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                early_stopping_counter = 0\n",
    "                self.save_model(path=self.config.trained_model_path, model=self.model)\n",
    "                self.logger.info(f'Saved model with valid loss: {valid_loss:.4f}')\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= self.config.early_stopping_patience:\n",
    "                    self.logger.info('Early stopping triggered')\n",
    "                    break\n",
    "\n",
    "        self.writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-23 13:46:16,969: INFO: utils: yaml file config/config.yaml loaded successfully:]\n",
      "[2024-07-23 13:46:16,971: INFO: utils: yaml file params.yaml loaded successfully:]\n",
      "[2024-07-23 13:46:16,972: INFO: utils: created directory at: artifacts:]\n",
      "[2024-07-23 13:46:16,973: INFO: utils: created directory at: artifacts/training:]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainingConfig' object has no attribute 'tensorboard_log_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m ConfigurationManager()\u001b[38;5;241m.\u001b[39mget_training_config()\n\u001b[0;32m----> 2\u001b[0m training \u001b[38;5;241m=\u001b[39m \u001b[43mTraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m training\u001b[38;5;241m.\u001b[39mget_base_model()\n\u001b[1;32m      4\u001b[0m training\u001b[38;5;241m.\u001b[39mtrain_valid_loader()\n",
      "Cell \u001b[0;32mIn[41], line 6\u001b[0m, in \u001b[0;36mTraining.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_logging()\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m SummaryWriter(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensorboard_log_dir\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m GradScaler()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainingConfig' object has no attribute 'tensorboard_log_dir'"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager().get_training_config()\n",
    "training = Training(config)\n",
    "training.get_base_model()\n",
    "training.train_valid_loader()\n",
    "training.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kidney",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
